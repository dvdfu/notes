ROM
---

ROM: read-only memory. There are different types of ROM:

* **Programmable ROM**: a fuse is blown with high voltage to write a 1. PROM can only be written once.
* **Erasable PROM**: can trap charges for decades, and erase data with UV light
* **Electrically Erasable PROM**: uses different levels of voltage to read, write, or erase. Any byte can be written. Writing can happen multiple times
* **Flash**: similar to EEPROM, but cheaper and only writes/erases in blocks of some size

Volatile Memory
---

Volatile memory loses data on power-off.

* **Static RAM**: fast, expensive, and has a large area (6 transistors). Used for cache memory. Writes by driving a bit line and its complement
* **Dynamic RAM**: slower than SRAM, and uses a periodic refresh. Has a smaller area than SRAM but is cheaper. Writes by driving bit line to voltage or ground.

Memory Chips
---

Suppose we have a "16x8" RAM (volatile memory: retains data on power-off). The memory chip contains 4 bits A<sub>0</sub>-A<sub>3</sub> to create 16-bit words, and 8 bits D<sub>0</sub>-D<sub>7</sub> for data. The chip also contains two input lines CS (chip select, active low) and WE (write enable, active low).

Large Memory Chips
---
e.g. 2M word x 8 bits (2<sup>21</sup> words x 8 bits/word) = 2<sup>24</sup> bit cells = 16 M bits = 4096 x 4096 bits. A 21x2097152 decoder would be slow.

Instead, arrange cells in a symmetric array (4096x4096) with 512 words per row. The row and column address share address pins (time-multiplexed). First the row address is given, then the column. Latches save those addresses and are controlled by the row address strobe (RAS) and the column address strobe (CAS).

Memory Metrics
---

**Latency**: the number of cycles from asserting the RAS until the first word is produced (time between asking for and receiving data).

**Bandwidth**: the volume of data that can be retrieved in a given unit of time. This differs from latency because more data can be requested at one time, but still arrive with the same delay in clock cycles.

**Asynchronous DRAM**: 2 words x 8 bytes/word x 200 MHz / 12 cycles = 267 MB/s

**Asynchronous Fast Page Mode DRAM**: 2 words x 8 bytes/word x 200 MHz / 9 cycles = 356 MB/s

**Synchronous DRAM**: 2 words x 8 bytes/word x 200 MHz / 7 cycles = 457 MB/s

**DDR SDRAM**: 2 words x 8 bytes/word x 200 MHz / 6 = 533 MB/s

Modules
---

1 GB DIMM with 64 M word x 8-bit chips

D<sub>63-56</sub> | D<sub>55-48</sub> | D<sub>47-40</sub> |  D<sub>39-32</sub> |  D<sub>31-24</sub> | D<sub>23-16</sub> | D<sub>15-8</sub> | D<sub>7-0</sub>
--- | --- | --- | --- | --- | --- | --- | ---
|

Volume = 64 MB / chip x 8 chip / side x 2 sides
<br>= 512 MB / side * 2 sides
<br>= 1 GB

**Dual-channel**: 2 DIMMs side-by-side, i.e. 128 bits in parallel

**Error checking code memory**: optionally included chip in the center of the memory module used to check internal data corruption.

Memory Hierarchy
---

Smallest, fastest memory available to the processor is located at the top of the memory hierarchy. The higher in the hierarchy, capacity decreases and cost per bit increases. Ideally, memory is both very large and very fast. Realistically, larger memories become proportionally slower.

memory | speed | capacity
--- | --- | ---
registers | 1 cycle | 4 bytes/register, 64-1024 B
cache L1 | ~2 cycles | ~64 kB
cache L2 | ~10 cycles | ~256 kB
cache L3 | ~40 cycles | ~8 MB
main memory | ~100 cycles | ~x GB
HHD / SSD | ~1-10 M cycles | ~x TB

Cache Memory
---

**Cache memory** is based on the idea that if you want a piece of data from memory, then:

* you'll want it again soon (temporal locality)
* you'll want nearby data (spatial locality)

Caches address these both by saving data (program data or instructions) that has been recently used and moving data from memory in blocks, taking advantage of higher bandwidth.

The ARM Cortex A9 uses blocks ("lines") of 8-word size. They are 4-way set-associative and critical word first.

Performance
---

Average memory access time:

t<sub>avg</sub> = h * C + (1 - h) * M

h = hit rate, (1 - h) = miss rate, C = cache hit time, M = miss penalty (time to load missing line).

increasing hit rate | decreasing cache hit time | decreasing miss penalty
--- | --- | ---
bigger cache | smaller cache | load-through: give data to processor while cache line is still loading
keep the right lines | | critical word first: memory sends block starting at requested address and then wraps to start
prefetch: load lines before needed | |

Store Instructions
---

1. **Write miss**: if the block of memory to store to is not in cache (missing), load the block into cache
1. Modify the cache line
1. **Write policy**: when to update memory
1. **Write-through**: modify memory at the same time (poor performance)
1. **Write-back**: mark line 'dirty', and write line to memory on eviction. Cache coherency problem: different caches may have different copies of the same block

Mapping Schemes
---

**Fully associative**: most flexible (highest hit rate), but must search all tags on every load, store, or instruction fetch. Takes 512-1024 comparators. expensive. All blocks in memory are stored in the cache level according to the block address mod 4 (giving 0-3).

| v (valid) | d (dirty) | tag | data (n words)
--- | --- | --- | --- | ---
L0 | | | |
L1 | | | |
L2 | | | |
L3 | | | |

**Direct-mapped**: line index = block number % number of lines. Least flexible, lowest hit rate, only searches one tag (least expensive).

**Set-associative**: set index = block number % number of sets. Within a set is fully associative. This is a compromise: decent hit rates without too much hardware.

Replacement Policy
---

If the cache is fully (fully associative) or the set is full (set associative), which line is replaced? **Least recently used** (LRU) has a better hit rate than FIFO or random.

Example: line size = 32 bytes, 64 lines, 16 address bits.

B0 address: 0-31: 0000 0000 000x xxxx
<br>B1 address: 32-63: 0000 0000 001x xxxx
<br>B2 address: 64-95: 0000 0000 010x xxxx

If address = 0x1234: the offset is bolded, the index is italicized, the rest is the tag.

full assoc. | 4-way set-assoc. | direct-mapped
--- | --- | ---
0001 0010 001**1 0100** | 0001 001<font color="red">**0 001**</font>**1 0100** | 0001 0<font color="red">**010 001**</font>**1 0100**

Virtual Memory
---

Two processes can share the same main memory but have separate address spaces.  Some of its address space is located on a hard disk. Advantages include larger overall memory.

Memory is mapped using **pages** - pages are usually around 4 kB. This takes advantage of hard disc bandwidth. We use a larger size because hard disc transfers have very high latency.<

Page Table 
---

Each process has a **page table**. This maps virtual pages to physical pages, which are called **page frames**. This is performed by the **memory management unit** (MMU).

Processor Design
---

**Execution Stages**

Stage | Data Processing | Load | Store | Branch
 --- | --- | --- | --- | ---
Instruction Fetch | ADD R2, R0, R1 | LDR R1, [R0, #4] | STR R1, [R0, #4] | BEQ [PC, #16]
Register Read | R0, R1 | R0 | R0, R1 | PC (not really)
Use ALU | [R0] + [R1] | [R0] + 4 | [R0] + 4 | [PC] + 16
Use Memory | - | read | write | -
Register Write | R2 &larr; | R1 &larr; | - | if (Z == 1) PC &larr;

Load Delay Slot
---

A **dependent instruction** (i.e., one that depends on the current instruction executing) following a load cannot get the load data for 1 cycle.

```
MOV r1, #3 ; a = #3
LDR r2, [r0, #4] ; b = [r0, #4]
ADD r3, r2, r1
```

clock | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8
--- | --- | --- | --- | --- | --- | --- | --- | ---
MOV r1, #3 | F | D | C | M (a) | W
LDR r2, [r0, #4] | | F | D | C | M | W (b)
ADD r3, r2, r1 | | | F | D | | C | M | W

The decode in the add instruction detects a load hazard. We can stall the dependent instruction (as above). Or we simply call it the **load delay slot** (retrieve the wrong value) and avoid the hazard by inserting an independent instruction in load delay slot.

```
LDR r2, [r0, #4] ; b = [r0, #4]
MOV r1, #3 ; a = #3
ADD r3, r2, r1
```
clock | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8
--- | --- | --- | --- | --- | --- | --- | --- | ---
LDR r2, [r0, #4] | F | D | C | M | W (b)
MOV r1, #3 | | F | D | C | M (a) | W
ADD r3, r2, r1 | | | F | D | C | M | W

Control Hazards
---

There could be hazards on the program counter caused by branches and jumps.

```
I1: B [pc, #x]
I2: ; arbitrary
I3: ; arbitrary
I1+x: ; arbitrary
I2+x: ; arbitrary
```

clock | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
--- | --- | --- | --- | --- | --- | --- | --- | --- | ---
I1: B [pc, #x] | F | D | C | (M) | (W)
I2 | | F | D | (C) | (M) | (W)
I3 | | | F | (D) | (C) | (M) | (W)
I1+x | | | | F | D | C | M | W
I2+x | | | | | F | D | C | M | W

The branch is taken in the 3rd clock cycle. The remainder of instruction I2/I3 are 'squashed' (cancelled)into **no-ops**. They are turned into useless functions that still trickle through the pipeline. Also observe that there is a two-cycle gap in cycles 6-7 where writeback is not performing any meaningful function.

We can reduce the penalty (stalls) to one cycle by inserting a dedicated branch adder to the decode stage. ~20% of the dynamic instruction mix are branches.

Branch Delay Slot
---

The instruction after the branch is always executed (never squashed). 

```
loop	LDR r3, [r1], #4
		ADD r0, r, r3 ; stalls one cycle because it is dependent on the LDR
		SUBS r2, r2, #1
		BGT loop
		NOP ; this is the branch delay slot, NOP does nothing
```

The above code uses 6 cycles the execute. We can reduce it to 4 cycles by moving the ADD instruction into the branch delay slot:

```
loop	LDR r3, [r1], #4
		SUBS r2, r2, #1
		BGT loop
		ADD r0, r, r3 ; still executed after the branch
```

Tutorial Example
---

|||||
---|---|---|---|---|---|---|---|---
LDR r3, [r1], #4|F|D|C|M|W
ADD r0, r0, r3||F|D|D|C|M|W
SUBS r2, r2, r1|||F|F|D|C|M|W
BGT [pc, #-16]|||||F|D
STR r0, r4||||||F|X
LDR|||||||F|D