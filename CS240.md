#Asymptotic Analysis

#Priority Queues

#Sorting

#Randomized Algorithms

#Dictionaries

#Balanced Search Trees

#Hashing

##Direct Addressing

Hashing supports **search, insert, and delete** in linear time. This is possible because keys are integers stored in an array.

Hashing is wasteful if the number of elements is significantly smaller than the max key value (array size).

A **hash function** is used to map objects to some integer key. The function is not typically injective; many keys can overlap, resulting in a **collision**.

##Load Factor

We want to address hashes that have a lot of collisions. The **load factor** ($\alpha$) of a hash is defined as
\[\alpha=\frac{n}{M}\]
where $n$ is the number of keys and $M$ is the array size. We want to restructure the hash when the load factor is not ideal. This is called **rehashing** and is done in $\Theta(M+n)$ time.

##Chaining

One strategy to resolve collisions is to store multiple values in a some ADT, called a **bucket**, for each key.

**Chaining** uses a linked list as the bucket. Assuming the hash is uniform, each linked list has an average size of $\alpha$.

|Operation|Average Case|Worst Case|
|---|---|---|
|search|$\Theta(1+\alpha)$|$\Theta(n)$|
|insert|$O(1)$|$O(1)$|
|delete|$\Theta(1+\alpha)$|$\Theta(n)$|

##Open Addressing

**Open addressing** is another strategy to resolve collisions. A key can be placed in multiple locations using a **probe sequence**. Think of open addressing as a hash function $h(k,i)$ that uses increasing $i$ values to find a valid location:
\[h(k,0),h(k,1),h(k,2),\dots\]

When searching for an element, each position in the probe sequence is checked. Search ends when all elements in the array are probed, or an empty location is found. Note that a deleted key is not the same as an empty location, and needs to be flagged as such.

In open addressing, $\alpha<1$ always.

##Linear Probing
The simplest probe sequence is a **linear probe**: if a location is occupied, check every following index in the array until the key can be stored.
\[h(k,i)=h(k)+i\text{ (mod } M)\]

|Operation|$\Theta$ Cost|
|---|---|
|search|$\frac{1}{(1-\alpha)^2}$|
|insert|$\frac{1}{(1-\alpha)^2}$|
|delete|$\frac{1}{1-\alpha}$|

##Double Hashing

**Double hashing** is a probe sequence used in open addressing. It uses two independent hashes $h_1$ and $h_2$ to calculate a valid location.
\[h(k,i)=h_1(k)+ih_2(k)\text{ (mod } M)\]

|Operation|$\Theta$ Cost|
|---|---|
|search|$\frac{1}{1-\alpha}$|
|insert|$\frac{1}{1-\alpha}$|
|delete|$\frac{1}{\alpha}\log{(\frac{1}{1-\alpha})}$|

##Cuckoo Hashing

**Cuckoo hashing** uses two independent hashes $h_1$ and $h_2$. A new key is always inserted to the position $h_1(k)$. In doing so it may need to 'evict' a previously existing key $j$, which is deferred to its $h_2(j)$ position, and the process repeats. If the array size is $n$ the process continues for over $n$ iterations, insertion failed and the elements need to be rehashed.

Cuckoo hashing requires $\alpha<\frac{1}{2}$.

|Operation|$\Theta$ Cost|
|---|---|
|search|$1$|
|insert|$\frac{\alpha}{(1-2\alpha)^2}$|
|delete|$1$|

##Hashing Functions

The **uniform hashing assumption** assumes all keys are equally likely to be hashed. This requires knowledge of input distribution. In general, good hashes are:

* easily computed
* unrelated to data patterns
* computed using every characteristic of the key

Some basic hash functions can be used for integer keys:

* **Division**: $h(k)=k\text{(mod }M)$ where $M$ is a prime and not near a power of 2
* **Multiplcation**: $h(k)=M(kA-\text{floor}(kA))$ where $0<A<1$, often $\phi$

More advanced input types (e.g. strings) are common. Remember that the hashing function time complexity is with respect to input size, not always a constant.

##Hashing vs. BST

|Hashing|BST|
|---|---|
|$O(1)$ average cost|$O(\log{n})$ worst cost|
|needs input knowledge|works for all comparable inputs|
|wasted space|no wasted space|
|rebuilt when $\alpha\geq1$|never rebuilt|

Both ADTs have methods to minimize page loads with external memory.

##Extendible Hashing

Very large dictionaries are stored using external memory using 'blocks' or 'pages'. Most hashing methods scatter data across many pages. The exception is linear probing, but it needs to avoid clustering by keeping $\alpha$ small, resulting in wasted space.

**Extendible hashing** is a strategy to hash a dictionary in external memory by keeping an in-memory 'catalog' of externally-stored blocks:

* keys range from $0\dots 2^L$, so they can be stored as L-length binary strings
* it contains an in-memory array called a **directory** of size $2^d$, where $d\leq{L}$
* $d=0$ when the directory is initialized, but grows as needed
* each directory entry points to a **block** in external memory that contains few enough items to store in one page of external memory
* the leading $d$ bits of a key in a block give us the directory index that points to the block
* multiple directory entries can point to the same block

Consider any block B. B stores a **local depth** $k_B\leq{d}$ which defines how many leading bits are common to all of its entries. All directory entries with the same $k_B$ leading bits point to B; there should be $2^{d-k_B}$ of them.

##Extendible Hashing - Operations

Searching for a key:

* compute the hash of the key: $h(k)$
* lookup the hash's leading d bits in the directory to find a block B
* load B from external memory
* find items in B that match $h(k$)
* among those items, find which match the key

If space exists in a block for item insertion, we simply insert it there. If full, we perform one of two operations.

**Block split**: a full block with $k_B<d$ implies multiple directory entires are pointing to B. We can use the $k_B+1$ bit to split items in B into two new blocks, each with a new local depth of $k_B+1$.

**Directory grow**: a full block with $k_B=d$ implies an item beyond the allowed value range is being inserted, or page sizes are too small. The size of the directory should be doubled and references should be updated. B should now be able to split.

Item deletion is performed in a reverse manner to insert, with **block merge** and **directory shrink** operations. It is possible to leave empty pages after a deletion with **lazy deletion**.

|Operation|$\Theta$ CPU time|...with directory change|Disk transfers|...with block change|
|---|---|---|---|---|
|search|$\log{S}$|-|1|-|
|insert|$S$|$2^d$|1|2|
|delete|$S$|$2^d$|1|2|

When considering using external memory for hashes, extendible hashing is better in nearly all aspects. The main disadvantage is CPU time complexity, but this is typically preferred over additional disk transfers.

|Hashing on external memory|Extendible hashing|
|---|---|
|potentially many disk transfers when probing|at most 2 disk transfers|
|rehashing requires moving all entries|rehashing adds new blocks and directory entries, but changes nothing existing|
|with uniform hashing, blocks are arbitrarily full|with uniform hashing, blocks are on average 69% full|
|$\Theta(1)$ average operation costs|$\Theta(\log{S})$ or $\Theta(S)$ average operation costs|

#Dictionaries

|Implementation|$\Theta$ search|$\Theta$ insert|$\Theta$ delete|
|---|---|---|---|
|unordered array|$n$|$1$|$n$|
|linked list|$n$|$1$|$n$|
|ordered array|$\log{n}$|$n$|$n$|
|BST|$\log{n}$|$\log{n}$|$\log{n}$|
|uniform hash|$1$|$1$|$1$|

##Interpolation Search

In an ordered array where keys are numbers, binary search can search for entries in $\log{n}$ time:

\[\text{binary search}(A[l,r], k):\text{ check }\frac{l+r}{2}=l+\frac{1}{2}(r-l)\]

**Interpolation search** is an extension of binary search that uses the difference two keys to guess the next location to check. The algorithm is based on the assumption that there is a linear distribution of values in the array, with an average time complexity of $O(\log\log{n})$. Its worst case performance is $O(n)$.

\[\text{interpolation search}(A[l,r], k):\text{ check }l+\frac{k-A[l]}{A[r]-A[l]}(r-l)\]

##Gallop Search

In an ordered array, it may not be feasible to determine the array length (data streams, large files). Then, we cannot directly access array elements using array length. **Gallop search** checks the first element of the array and uses exponentially larger indexes (0, 1, 2, 4, 8, 16...). Gallop search defers to binary search once it overshoots the key and finds its upper-bound index.

Gallop search runs in $O\log{m}$ time, where $m$ is the position of the key in the array. There are $n$ elements in the array, and $m\leq{n}$.

##Self-Organizing Search

Unordered linked list have linear search time complexities. If elements in the linked list are not uniformly accessed (i.e. some are searched more frequently than others), we can use **self-organizing search** to arrange elements based on access probability. More frequently accessed elements can be moved to the front of the array to lower the expected cost of searching.

If access probabilities are not known in advance, we can use **dynamic ordering** heuristics. These methods work for both arrays and linked lists.

##Move to Front

**Move to Front** (MTF) is a self-organizing heuristic where a successfully searched entry is moved to the front of the list. This tends to work well and is no more than twice as bad as **static ordering** (no items are moved), that is:

\[C_{MTF}=L^2\]
\[C_{Opt}=\frac{L(L+1)}{2}\]
\[\frac{C_{MTF}}{C_{Opt}}\approx{2}\]
Some variations of MTF include MTB (move to back) or move halfway to front.

##Transpose

Another self-roganizing heuristic where a successfully searched entry is swapped with the entry preceding it. This can have very poor performance with certain access probabilities.

##Skip Lists

**Skip lists** are a randomized ADT that contains a series of lists:

* a set $S$ has a skip list $S_0, S_1, S_2,\dots,S_h$
* each list is a sub-list of the previous list and stores no, some, or all of the elements in $S$ in non-decreasing order
* $S_0$ contains every element in $S$, $S_h$ contains none of them
* each list has two special keys $-\infty$ and $+\infty$ at the beginning, respectively

In a set of size $n$, a skip list is effectively a 2-dimensional $n\times{h}$ grid of keys. Because higher level lists contain fewer keys, search is done by traversing higher level lists and then deferring to lower levels, minimizing checks. Any values in higher-level skip lists have pointers to the value represented in the adjacent lower list (if it exists), and to the next element with the same height.

The empty-top level skip list and $\infty$ pointers are used so that elements are always bounded by a lesser and greater value. The elements that are included in higher levels of the skip list are determined at random during insertion. For every insertion, a random height is determined:

|Height|Likelihood|
|---|---|
|$0$|$0.5$|
|$1$|$0.25$|
|$2$|$0.125$|
|$3$|$0.0625$|
|$i$|$0.5^{i+1}$|

This can be thought of as the number of heads flipped on a coin before flipping tails. An insertion of height $i$ inserts an entry to every list $S_0\dots{S_i}$ in the correct ordered position. This may cause the total height of the skip list to increase.

Skip lists are fast and simple to implement in practice. They have $O(n)$ expected space, $O(\log{n})$ expected height, and $O(\log{n})$ expected time for all operations. A skip list with $n$ items has height at most $3\log{n}$ with probability at least $1-1/n^2$.

#Multi-Dimensional Data Dictionaries

**Multi-dimensional** data refers to collections of objects that have multiple **dimensions** (e.g. an employee can be 3-dimensional: name, age, salary). Each dimension $d$ has an **aspect** (coordinate). Search involves specifying a set of dimensions and a desired range for each dimension.

##Range Search

**Range dictionaries** support insert, delete and **range search**. Range search returns entry sets that specify some query (e.g. an integer array for values between 1, 5).

Ordered array range search requires $O(\log{n}+k)$ time, where $k$ is the number of reported items. It is preferable to use a sorted tree ADT. Hashing cannot be used as a ranged dictionary.

In an AVL tree range search, two paths are taken to find leaf nodes that bound the desired range.

* $P_L$: path from the root to a leaf that goes right if $k<k_1$ and left otherwise
* $P_R$: path from the root to a leaf that goes left if $k>k_2$ and right otherwise

This produces 3 sets of node characteristics:

* **boundary nodes**: traversed by $P_L$ or $P_R$. Possibly desired
* **inside nodes**: non-boundary nodes from right-subtrees of $P_L$ nodes or left-subtrees of $P_R$ nodes. Desired
* **outside nodes**: non-boundary nodes from left-subtrees of $P_L$ nodes or right-subtrees of $P_R$ nodes. Ignored

If $k$ items are reported, then AVL tree range search visits $\log{n}$ boundary nodes and $k$ inside nodes, producing a runtime of $O(\log{n}+k)$, which is considerably better than ordered array range search.

$d=2$ collections have objects as points on a Euclidean plane. Searching is comparable to finding points within some rectangle. Using two 1-dimensional range searches requires having duplicate collections that are sorted differently, which is space-inefficient. The solution is to use a **partition tree**:

* tree with $n$ leaves, each corresponding to an item
* internal nodes represent a region

##Quadtrees

A quadtree has $n$ points in the set $P=\{(x_0,y_0)\dots(x_{n-1},y_{n-1})\}$. To build a quadtree:

* find a square $R$ that contains all points by determining minimum/maximum x/y coordinates in linear time
* setting the quadtree root as the region $R$
* **split**: partition $R$ into 4 equal **quadrants**; each quadrant region is a child of its original square. $R$ is usually assigned power-of-2 dimensions initially for easy splitting
* recursively split a quadrant until it contains 1 or no points. Empty quadrant nodes can be deleted, so every quadrant has exactly 1 point
* points on quadrant boundaries belong to the left/bottom side

Quadtree operations:

* search: analogous to binary search trees
* insertion: searches for the position and splits the quadrant if it is non-empty
* deletion: searches for the point, removes the point, walks up the tree, and deletes any unnecessary splits that may occur after the point is removed

Range search in a quadtree uses a simple recursive algorithm:

* specify a rectangle $R$ in the tree $T$
* if $T$ is a leaf, report its point if it is in $R$
* if $T$ is a node, recursively search a subtrees if its rectangle overlaps with $R$

The **spread factor** $\beta$ of a set of points $P$ is defined as
\[\beta(P)=\frac{d_{max}}{d_{min}}\]
where $d_{max}, d_{min}$ are the maximum and minimum distances between any two points in $P$, respectively. The height of a quadtree is $h\in\Theta(\log{\beta})$.

* worst-case build time complexity: $\Theta(nh)$
* worst-case range search time complexity: $O(nh)$

|Advantages|Disadvantages|
|---|---|
|easy handling, computation|space-wasteful|
|no complicated arithmetic|bad point distributions lead to very large height|
|easily generalizable to higher dimensions (3-dimensional: octree)||

##KD Trees

**KD trees** are used in similar situations as quadtrees. Instead of dividing regions into equally-sized quadrants, it splits regions into 2 subsets of roughly equal quantities of points, analogous to a balanced binary tree, where every node is a **split** on a point (internal node) or a point (leaf node).

Points are sorted by their x-component and split through their median with a vertical line. Then, points in the subsets are sorted by y-component and split through their median with a horizontal line. The veritcal-horizontal splits repeat until every subset has only 1 point. This results in a $\Theta(\log{n})$-height tree constructed in $\Theta(n\log{n})$ time.

KD tree range search is done in $O(k+U)$ where $k$ is the number of reported items and $U$ is the number of unsuccessful region visits.

#Tries

A **trie** (retrieval) or **radix tree** is a binary string dictionary. It uses bitwise comparison and is similar to radix sort. A left child and right child correspond to a 0 and 1 bit, respectively. Keys are not stored in the tree - a node is flagged if the path from the root to itself generates a binary string present in the dictionary. A trie can perform all operations in $\Theta(|x|)$ time, where $|x|$ is the length of the string.

A trie search begins at the root and follows the path created by the query string. If the path terminates on a flagged node, the string is found.

A trie insert follows this algorithm:

* search for $x$
* if search terminates successfully, do nothing
* if search terminates on an internal node with extra bits in $x$, add nodes to create the path to $x$
* if search terminates on an internal node with no extra bits in $x$, flag the node
* if search terminates on a leaf with extra bits in $x$, add nodes to create the path to $x$

A trie delete follows this algorithm:

* search for $x$
* if search terminates unsuccessfully, do nothing
* if search terminates on an internal node, unflag the node
* if search terminates on a leaf, delete all ancestors until we reach an ancestor with two children, or a flagged node

##Compressed Tries

A **PATRICIA** trie is a Practical Algorithm To Retrieve Information Coded In Alphanumeric. This trie compresses sequences of unflagged node paths that only have one child by storing an **index** in each node that indicates the next bit tested in a search. A compressed trie that stores $n$ keys always has at most $n-1$ internal nodes.

##Multiway Tries

**Multiway** tries are identical to regular tries, but the strings can be over any fixed alphabet $\Sigma$, and every node has up to $|\Sigma|$ children. A special **end-of-word** character (say, $) is added to all keys. Multiway tries can also be compressed using PATRICIA.

#Pattern Matching

**Pattern matching** is a process that involves searching a large body of text $T[0\dots{n-1}]$ (**haystack**) for a pattern $P[0\dots{m-1}]$ (**needle**) over an alphabet $\Sigma$. It returns the first $i$ such that $P[j]=T[i+j]$ for $0\leq{j}\leq{m-1}$, so that it is the first occurrence of $P$ in $T$ (or fail).

**Substring**: $T[i\dots{j}],0\leq{i}\leq{j}<n$, a string of length $j-i+1$ which consists of characters $T[i],\dots{}T[j]$ in order.

**Prefix**: a substring $T[0\dots{}i]$ of T for some $0\leq{i}<n$.

**Suffix**: a substring $T[i\dots{}n-1]$ of T for some $0\leq{i}\leq{n-1}$.

A **guess** is a position $i$ such that $P$ might start at $T[i]$. Valid guesses (initially) are $0 \leq{i}\leq{n-m}$.

A **check** of a guess is a single position $j$ with $0\leq{j}<m$ where we compare $T[i+j]$ to $P[j]$. We must perform $m$ checks of a single correct guess, but may make (many) fewer checks of an incorrect guess.

Example: $T=abbbababbab$, $P=abba$

|a|b|b|b|a|b|a|b|b|a|b|
|---|---|---|---|---|---|---|---|---|---|---|
|a|b|b|a||||||||
||a||||||||||
|||a|||||||||
||||a||||||||
|||||a|b|b|||||
||||||a||||||
|||||||a|b|b|a||

The worst possible input is $P=a^{m-1}b$, $T=a^n$. The worst case performance is $\Theta((n-m+1)m)$, or $\Theta(mn)$ when $m\leq{n/2}$. We use preprocessing to eliminate guesses based on completed matches and mismatches in more sophisticated algorithms.

##KMP

Search patterns left-to-right, but shifts the pattern possibly more than one index on a mismatch. **KMP** uses a **failure array** $F$ to determine the first position the pattern can 'slide' to after finding a mismatch. $F$ is defined as:

* $F[0]=0$
* $F[j]$ for $j>0$ is the length of the largest prefix of $P[0\dots{j}]$ that is also a suffix of $P[1\dots{j}]$

If a match fails at index $j$ of our pattern, then we look at $F[j-1]$. We can continue looking for the pattern using the $F[j-1]^{th}$ element of the failure array. If $j=0$ then we advance the pattern in our text.

The failure array takes no more than $2m$ iterations to compute, so it has a linear runtime. The KMP algorithm takes no more than $2n$ iteration, so it also has a linear runtime.

##Boyer-Moore

##Suffix Trees

#Compression

**Source text** $S$ is original data stored using a **source alphabet** $\Sigma_S$. **Coded text** $C$ is encoded source text using a **coded alphabet** $\Sigma_C$, usually binary. Encoding/decoding algorithms can be measured:

* Processing speed
* Reliability (error-correcting)
* Security
* Size

In most cases, we want to **compress** so that the size $|C|$ of our coded text is minimized. This is measured using the **compression ratio**:

\[\frac{|C|\cdot{}\log|\Sigma_C|}{|S|\cdot{}\log|\Sigma_S|}\]

##Compression Types

**Logical compression** is aware of the meaning represented by the data, and only applies to compression of specific domains.

**Physical compression** is unaware of the meaning of the data, and performs compression purely based on the bits of the data.

**Lossy compression**, achieves better compression ratios at the expense of not necessarily being able to recover the exact source text $S$ from $C$.

**Lossless compression** can always decode source text $S$ exactly from $C$.

Lossy, logical compression is great for media files. Lossless, physical compression work for any application and will be the focus of the course.

##Encoding

**Character encodings** provide an exact matching from the source alphabet to binary strings. In general, we use a **decoding dictionary** $D$ to map coded text to source text:

* must be **prefix-free**
* may be used and stored explicitly (e.g. as a trie) or implicitly
    * **fixed**: agreed in advance
    * **static**: stored in the message
    * **adaptive**: stored implicitly

**ASCII** encoding is a character encoding that matches 7-bit patterns a table of 128 letters, numbers, and symbols. It is a **fixed-length code** because every key string has the same length. We decode ASCII by looking up a 7-bit pattern in the table, so the decoding dictionary is fixed.

**UTF-8** encoding is a **variable-length code** that uses 1-4 bytes to encode more than 107,000 characters. UTF-8 also used a fixed decoding dictionary, and every UTF-8 character is formatted with:

* a 0 prefix to indicate ASCII, followed by 7 data bits
* 1-4 prefixed 1's to indicate the length of the character in bytes, followed by a 0, and 3-5 data bits. The next one to four bytes each begin with 01, followed by 6 data bits

```
ASCII: 0000 0000 - 0000 007F
0-------

2 bytes: 0000 0080 - 0000 07FF
110----- 10------

3 bytes: 0000 0800 - 0000 FFFF
1110---- 10------ 10------

4 bytes: 0001 0000 - 0010 FFFF
11110--- 10------ 10------ 10------
```

Some characters in our alphabet occur more frequently than others. To maximize compression, we should allocate codes of shorter length to character that occur more frequently. Obviously, this only applies to variable-length codes.

##Huffman Encoding

In **Huffman Encoding**, an arbitrary source alphabet is coded using $\{0,1\}$ (binary strings). We store the decoding dictionary in a binary trie, where all leaves are members of the source alphabet, and the trie path represents that letter's coding. Huffman encoding uses a static dictionary: the dictionary is the same for the entire encoding/decoding, and is not fixed.

The 'best' trie minimizes the length of $C$. We can build the best trie by putting each character into its own trie, and merging tries in order of their character's frequencies:

* determine the frequency of every character in the alphabet
* make a height-0 trie for every character
* determine a 'weight' for every trie (its frequency), and store all tries in a min-heap
    * retrieve the two minimum entries in the min-heap
    * merge them into a new trie (summing their weights)
    * insert the new trie back into the min-heap, repeating until one trie is left
* this decoding trie is transmitted along with a coded text $C$

With Huffman coding, the encoder has to do a lot of work. Decoding is faster, so this is an **asymmetric scheme**. If we wish to encode one character at a time, Huffman coding is optimal.

##Run-Length Encoding

In **Run-Length Encoding** (RLE), both the source and coded alphabet are binary. This encoding is not a character encoding, so multiple character can be represented by a single dictionary entry. RLE uses a fixed dictionary.

A sequence of consecutive 0s or 1s is called a **run**. We encode source text by indicating the first bit of $S$ (0 or 1), and then a sequence of integers $k$ that represent lengths of runs.

```
source: 00000111101100011
runs:   00000 1111 0 11 000 11
k:      5     4    1 2  3   2
binary: 101   100  1 10 11  10
```

$k$ can be arbitrarily long, so the decoder needs to know when to start/stop reading a single $k$. We encode $k$ using:

* $\lfloor\log{k}\rfloor$ (the length of the $k$ string, minus 1) copies of 0, followed by
* the binary representation of $k$

```
source: 1111111001000000000000000111111
runs:   1111111 00  1 000000000000000 111111
k:      7       2   1 15              6
binary: 111     10  1 1111            110
code:   00111   010 1 0001111         00110
        0001110101000111100110
```

A run is not compressed unless $k\geq{6}$. For $k=2,4$, RLE **expands** the data. RLE is only really useful for some kinds of data.

##Adaptive Dictionaries

**Adaptive coding** uses some initial (usually fixed) dictionary $D_0$. $D_i$ is used for $i\geq{0}$ to determine the $i^{th}$ output character. After writing the $i^{th}$ character, both encoder and decoder update $D_i$ to $D_{i+1}$. Both encoder and decoder must have the same information, and usually have the same cost.

##Lempel-Ziv

**Lempel-Ziv** is a family of adaptive compression algorithms. Coded characters refer to either single characters in $\Sigma_S$ or substrings of $S$. Lempel-Ziv algorithms include variants such as LZ77 and LZ78, which are used in formats and tools such as `gzip`, `gif`, `compress`, and `png`. These algorithms take advantage of the fact that certain substrings are much more frequent than others:

* English: `th`, `er`, `on`, `an`, `re`, `the`, `and`, `tha`...
* HTML: `<a href`, `<img src`, `<div`...
* Video: unchanged pixels between frames, shifted sub-images

###Lempel-Ziv-Welch

**Lempel-Ziv-Welch** (LZW) is a fixed-width encoding using $k$ bits that stores a decoding dictionary $D$ with $2^k$ entries. $D$ initially contains the first $|\Sigma_S|$ entries for single characters. The remaining entries are substrings of $S$.

If substring $y\in{S}$ is encoded, then $xc$ is added to $D$, where $x$ is the previously encoded substring of $S$, and $c$ is the first character of $y$.

```c++
// LZW encode
x = NULL
while there are more symbols in S:
    c = next symbol from S
    if xc exists in D:
        x = xc
    else:
        output index(x)
        add xc to D
        x = c
;
```

The following example shows LZW encoding where $k\geq{}8$, $\Sigma_S$ is ASCII, and the input to be encoded is `YO! YOU! YOUR YOYO!`.

|Read|Lookup|Add|C|
|---|---|---|---|
|`Y`|89||89|
|`O`|79|128 `YO`|89 79|
|`!`|33|129 `O!`|89 79 33|
|`_`|32|130 `!_`|89 79 33 32|
|`YO`|128|131 `_Y`|89 79 33 32 128|
|`U`|85|132 `YOU`|89 79 33 32 128 85|
|`!_`|130|133 `U!`|89 79 33 32 128 85 130|
|`YOU`|132|134 `!_Y`|89 79 33 32 128 85 130 132|
|`R`|82|135 `YOUR`|89 79 33 32 128 85 130 132 82|
|`_Y`|131|136 `R_`|89 79 33 32 128 85 130 132 82 131|
|`O`|79|137 `_YO`|89 79 33 32 128 85 130 132 82 131 79|
|`YO`|128|138 `OY`|89 79 33 32 128 85 130 132 82 131 79 128|
|`!`|33|139 `YO!`|89 79 33 32 128 85 130 132 82 131 79 128 33|

LZW decoding works in a similar way, and builds a dictionary while reading strings symbol by symbol. The decoder is generally 'one step behind' when creating $D$, so it is possible it will try to use a code that it is about to build.

```c++
// LZW decode
D = ASCII dictionary
i = 128
code = next symbol from S
s = D(code)
output s
while there are more symbols in S:
    s_ = s
    code = next symbol from S
    if code == i:
        s = s_ + s_[0]
    else
        s = D(code)
    D.insert(i, s_+s[0])
    i++
;
```

The following example shows LZW decoding on the coded text 1030489, with the intial dictionary mapping { 0, 1, 2, 3 } to { A, B, C, D }.

|Code|Lookup|Add|Source|
|---|---|---|---|
|1|B|4 BA|B|
|0|A|5 AD|B A|
|3|D|6 DA|B A D|
|0|A|7 AB|B A D A|
|4|BA|8 BAB|B A D A BA|
|8|BAB|9 BABB*|B A D A BA BAB|
|9|BABB|-|B A D A BA BAB BABB|

The source string BABB is determined using $s+s[0]$, where $s$ is the previous string added to the dictionary, BAB.

##Summary

|Huffman|Run-Length|Lempel-Ziv-Welch|
|---|---|---|
|variable-length|variable-length|fixed-length|
|single-character|multi-character|multi-character|
|2-pass|1-pass|1-pass|
|60% compression (English)|bad on text|45% compression (English)|
|must send dictionary|can be worse than ASCII|can be worse than ASCII|
|used partially in JPEG, MP3|use partially in fax, images|used often in GIF|

#Text Transformation

Efficient compression needs frequent character/substring patterns. We can first modify the text so these are likely.

##Move to Front

**Move to Front** is an adaptive compression algorithm that takes advantage of data locality. Its dictionary is stored as an unsorted linked list where accessed elements are move to the front. The coded alphabet $\Sigma_C$ will be the set of numbers $\{0,1,2,\dots{}m-1\}$ where $m$ is the size of the source alphabet.

```c++
// MTF encode
L = linked list of source alphabet in an agreed, fixed order
while there are more symbols in S:
    c = next symbol from S
    output index(i) where L[i] = c
    move c to L[0]

// MTF decode
L = linked list of source alphabet in an agreed, fixed order
while there are more integers in C:
    i = next integer from C
    output L[i]
    move L[i] to L[0]
;
```

This example shows how the source text CCTGAT from the alphabet ACGT is encoded with the initial dictionary ordered as ACGT.

|D|S symbol|i|C|
|---|---|---|---|
|ACGT|C|1|1|
|CAGT|C|0|10|
|CAGT|T|3|103|
|TCAG|G|3|1033|
|GTCA|A|3|10333|
|AGTC|T|2|103332|

A run in $S$ can be seen in $C$ as a sequence of 0s. A run in $C$ implies a possibly repeated substring sequence in $S$.

Notice that $|C|=|S|$, so MTF does not provide compression on its own. However, it converts arbitrary data into an integer sequence, which can then be compressed. We can compress $C$ using RLE or Huffman Encoding.

##Burrows-Wheeler Transform

The **Burrows-Wheeler Transform** (BWT) encodes source text by permuting it so that MTF compression is easier. BWT is a block compression method, and decoding is more efficient than encoding (asymmetric scheme). BWT compresses English better than other algorithms mentioned.

BWT assumes the source text $S$ ends with an **end-of-word** character (\$) that appears nowhere else in $S$. Then:

* all **cyclic shifts** of $S$ are put into list $L$
* $L$ is sorted lexicographically, with \$ preceding all other characters
* $C$ is the list of trailing characters of each $L$ string

The following example shows how the source text 'SMASH' is encoded using BWT:

```
cyclic shifts:
SMASH$
MASH$S
ASH$SM
SH$SMA
H$SMAS
$SMASH

sorted shifts:
$SMASH
ASH$SM
H$SMAS
MASH$S
SH$SMA
SMASH$

coded text (BWT output):
012345
HMSSA$

coded text, sorted lexicographically, index-preserved:
540123
$AHMSS

source text, following index order:
531420
SMASH$
```
